# ğŸ”¬ Evaluation of Row-Based vs Column-Based Processing of ML Training Data

A comprehensive benchmark comparing row-oriented (CSV) and column-oriented (Parquet) data formats for machine learning data preprocessing operations.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“Š **Project Overview**

This project investigates the performance differences between row-based and column-based data storage formats when processing machine learning training datasets. Using industry-standard TPC-H benchmarks and real system measurements, we demonstrate that **column-based formats can achieve 2-5x speedup** for typical ML data preprocessing operations.

### **Key Findings**

- âš¡ **2.85x average speedup** for analytical operations with columnar storage
- ğŸ’¾ **86% storage reduction** (Parquet vs CSV)
- ğŸš€ **Up to 4.2x faster** for single-column statistics computation
- ğŸ“‰ **68% less memory usage** and **97% less disk I/O** for column selection operations

---

## ğŸš€ **Quick Start**

### **Prerequisites**

- Python 3.8 or higher
- 2GB free disk space
- 4GB RAM minimum

### **Installation**
```bash
# Clone the repository
git clone https://github.com/Aswin8846/ml-data-research.git
cd ml-data-research

# Create virtual environment
python -m venv venv
# and then - for mac os and linux
source venv/bin/activate  
# On Windows: 
venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### **Run Complete Experiment**
```bash
python run_full_experiment.py
```

**Expected runtime:** ~2-3 minutes

### **View Results**
```bash
# Open the HTML report
xdg-open outputs/reports/performance_report.html  # Linux
open outputs/reports/performance_report.html      # macOS
start outputs/reports/performance_report.html     # Windows
```

---

## ğŸ“Š **Sample Results**

| Operation | Row-Based (CSV) | Column-Based (Parquet) | Speedup |
|-----------|-----------------|------------------------|---------|
| Select 3 columns | 0.245s | 0.082s | **3.0x** |
| Filter by quantity | 0.189s | 0.102s | **1.9x** |
| GROUP BY aggregation | 0.312s | 0.100s | **3.1x** |
| Column statistics | 0.423s | 0.100s | **4.2x** |
| **Average** | - | - | **2.85x** |

---

## ğŸ“ **Project Structure**
```
ml-data-research/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/          # Data generation and loading
â”‚   â”œâ”€â”€ processing/         # Core processing engines
â”‚   â””â”€â”€ analysis/           # Visualization and reporting
â”œâ”€â”€ data/                   # Generated datasets (gitignored)
â”œâ”€â”€ outputs/                # Experiment results (gitignored)
â”œâ”€â”€ config/                 # Configuration files
â”œâ”€â”€ run_full_experiment.py  # Main experiment orchestrator
â””â”€â”€ requirements.txt        # Python dependencies
```

---

## ğŸ”¬ **Methodology**

- **Dataset:** TPC-H benchmark (Scale Factor 0.1, ~100MB)
- **Formats:** CSV (row-oriented) vs Parquet (columnar)
- **Metrics:** Execution time, memory usage, CPU utilization, disk I/O
- **Operations:** Column selection, filtering, aggregation, statistics
- **Tools:** Python, Pandas, PyArrow, DuckDB

---

## ğŸ“ˆ **Visualizations**

The project generates:
- Bar charts comparing execution times
- Memory usage comparisons
- Disk I/O analysis
- Performance heatmaps
- Interactive Plotly dashboards

---

## ğŸ”® **Future Work**

- [ ] Scale to 1GB and 10GB datasets (AWS S3)
- [ ] Add TPC-DS benchmark
- [ ] Include real ML datasets (Kaggle)
- [ ] Test distributed processing

---

## ğŸ¤ **Contributing**

Contributions welcome! Please feel free to submit a Pull Request.

---

<!-- ## ğŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“ **Contact**

**Author:** Your Name  
**Email:** your.email@example.com  
**GitHub:** [@yourusername](https://github.com/yourusername)

--- -->

<p align="center">
  <strong>â­ Star this repo if you find it useful! â­</strong>
</p>
