# ML Data Research - Master Guide

Complete guide covering setup, usage, features, and Hetzner integration.

---

## Quick Start (2 Commands)

```bash
# 1. Install dependencies
uv sync

# 2. Run experiment
uv run python run_full_experiment.py --processor duckdb
```

---

## Installation & Setup

### Install uv (One-time)
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### Install Dependencies
```bash
cd ml-data-research
uv sync
```

### Run Commands
```bash
# Method 1: Direct (recommended)
uv run python run_full_experiment.py

# Method 2: Activate venv manually
source .venv/bin/activate
python run_full_experiment.py
```

---

## What You Have

### Step 2: DuckDB Processor (10-100x faster than Pandas)
```python
from src.processing.duckdb_processor import DuckDBProcessor
from pathlib import Path

proc = DuckDBProcessor(Path("./data/raw/tpc_h_sf0.1/parquet"))
proc.register_all_tables()

# Statistics (100x faster!)
stats = proc.compute_statistics("lineitem", "l_extendedprice")

# Filtering (vectorized, 6x faster)
result = proc.filter_rows("lineitem", "l_quantity > 30")

# Aggregation (10x faster)
result = proc.aggregate_rows("lineitem", ["l_returnflag"], {"l_quantity": "sum"})
```

### Step 3: Chunked Processor (Unlimited file size)
```python
from src.processing.chunked_processor import LocalChunkedProcessor

proc = LocalChunkedProcessor(Path("./data/raw/tpc_h_sf0.1/parquet"))

# Stream in batches - constant memory regardless of file size
for batch in proc.read_batches("lineitem", columns=["l_quantity"]):
    filtered = batch[batch['l_quantity'] > 30]
    print(f"Batch: {len(filtered)} rows")
```

### Step 4: Enhanced Experiment Runner (Multi-processor support)
```bash
# Default (100MB, DuckDB)
uv run python run_full_experiment.py

# With options
uv run python run_full_experiment.py \
    --processor duckdb \
    --scale-factor 1 \
    --chunk-size 100000 \
    --memory-limit 2048
```

### Step 1: Hetzner Integration (Multi-format support)
```python
from src.ingestion.remote_storage import HetznerStorage

storage = HetznerStorage(
    endpoint="https://fsn1-1.your-hetzner.storage.api/",
    access_key="YOUR_KEY",
    secret_key="YOUR_SECRET",
    bucket="ml-data-research"
)

# Auto-detect format and stream
for batch in storage.read_auto_format("tpc_h_sf10/lineitem.tbl"):
    print(f"{len(batch)} rows")

# Or be specific
for batch in storage.read_parquet_batch("tpc_h_sf10/lineitem.parquet"):
    pass

for batch in storage.read_tbl_batch("tpc_h_sf10/lineitem.tbl"):
    pass

for batch in storage.read_dat_batch("tpc_h_sf10/lineitem.dat", delimiter='|'):
    pass
```

---

## Supported Data Formats

All formats support streaming - **no memory limits**!

### Parquet (.parquet)
- **Best for:** Performance, compression
- **Size:** 2-3GB (10GB data)
- **Speed:** ‚ö°‚ö°‚ö° Fastest
- **Memory:** 50MB/batch
- **Compression:** 70% reduction

```python
for batch in storage.read_parquet_batch("lineitem.parquet"):
    print(f"{len(batch)} rows")
```

### TPC-H TBL (.tbl)
- **Best for:** TPC-H benchmarks, compatibility
- **Format:** Pipe-delimited (`|`)
- **Size:** 10GB
- **Speed:** ‚ö°‚ö° Fast
- **Memory:** 100MB/batch

```python
for batch in storage.read_tbl_batch("lineitem.tbl"):
    print(f"{len(batch)} rows")
```

### DAT (.dat) / CSV (.csv)
- **Best for:** Flexibility, custom delimiters
- **Format:** Any delimiter (comma, pipe, etc.)
- **Size:** 10-12GB
- **Speed:** ‚ö° Medium
- **Memory:** 100-120MB/batch

```python
# Custom delimiter
for batch in storage.read_dat_batch("lineitem.dat", delimiter='|'):
    print(f"{len(batch)} rows")

# CSV
for batch in storage.read_dat_batch("data.csv"):
    print(f"{len(batch)} rows")
```

### Auto-Detection
```python
# Automatically detects format by extension
for batch in storage.read_auto_format("lineitem.tbl"):
    print(f"{len(batch)} rows")
```

---

## Configuration

### YAML Config (config/processing.yaml)
```yaml
processing:
  processor: "duckdb"          # pandas, duckdb, chunked, remote
  scale_factor: 0.1            # 0.1=100MB, 1=10GB, 10=100GB
  chunk_size: 100000           # Rows per batch
  max_memory_mb: 2048          # Memory limit
  
  duckdb:
    in_memory_db: true
    memory_limit_gb: 16
    threads: 4

hetzner:
  enabled: false               # Set to true when you have credentials
  endpoint: "${HETZNER_ENDPOINT}"
  access_key: "${HETZNER_ACCESS_KEY}"
  secret_key: "${HETZNER_SECRET_KEY}"
  bucket: "ml-data-research"
```

### Environment Variables
```bash
export APP_CONFIG_PROCESSING_PROCESSOR=chunked
export APP_CONFIG_PROCESSING_SCALE_FACTOR=1
export APP_CONFIG_PROCESSING_MAX_MEMORY_MB=4096

# Then run
uv run python run_full_experiment.py
```

### Hetzner Setup (.env)
```bash
cat > .env << 'EOF'
HETZNER_ENDPOINT="https://fsn1-1.your-hetzner.storage.api/"
HETZNER_ACCESS_KEY="your_key"
HETZNER_SECRET_KEY="your_secret"
HETZNER_BUCKET="ml-data-research"
EOF
```

Then enable in `config/processing.yaml`: `hetzner.enabled: true`

---

## Running Experiments

### Basic Commands

```bash
# Run benchmark from Hetzner (with credentials in .env)
uv run python run_full_experiment.py \
    --scale-factor 10 \
    --data-source hetzner \
    --processor chunked

# Or use DuckDB for faster processing
uv run python run_full_experiment.py \
    --scale-factor 10 \
    --data-source hetzner \
    --processor duckdb

# Help
uv run python run_full_experiment.py --help
```

### Processor Options
```bash
# DuckDB - SQL engine (10-100x faster)
--processor duckdb

# Chunked - Streaming (unlimited size)
--processor chunked

# Pandas - Original (slow, limited memory)
--processor pandas

# Remote - From Hetzner S3
--data-source hetzner
```

### Memory Control
```bash
# Set chunk size
--chunk-size 50000

# Set memory limit
--memory-limit 1024
```

---

## Testing

### Install Test Dependencies
```bash
uv pip install pytest pytest-benchmark
```

### Run Tests
```bash
# All tests
uv run pytest test_scaling.py -v

# Fast tests only (skip benchmarks)
uv run pytest test_scaling.py -v -m "not benchmark"

# Specific test
uv run pytest test_scaling.py::TestDuckDBProcessor::test_statistics -v
```

### Test Coverage
- 7 DuckDB tests
- 5 Chunked processor tests
- 2 Processor equivalence tests
- 5 Configuration tests
- 2 Hetzner tests
- 2 Performance benchmarks

---

## Code Examples

### Direct DuckDB Usage
```python
from src.processing.duckdb_processor import DuckDBProcessor
from pathlib import Path

proc = DuckDBProcessor(Path("./data/raw/tpc_h_sf0.1/parquet"))
proc.register_parquet_table("lineitem")

# Query
result = proc.to_pandas(proc.filter_rows("lineitem", "l_quantity > 30"))
print(f"Filtered: {len(result)} rows")
```

### Direct Chunked Usage
```python
from src.processing.chunked_processor import LocalChunkedProcessor
from pathlib import Path

proc = LocalChunkedProcessor(Path("./data/raw/tpc_h_sf0.1/parquet"))

# Stream batches
for batch in proc.read_batches("lineitem", columns=["l_quantity"]):
    print(f"Processing {len(batch)} rows")
```

### Direct Hetzner Usage
```python
from src.ingestion.remote_storage import HetznerStorage

storage = HetznerStorage(
    endpoint="https://...",
    access_key="KEY",
    secret_key="SECRET",
    bucket="ml-data-research"
)

# List available datasets
datasets = storage.list_datasets()
for name, info in datasets.items():
    print(f"{name}: {info['total_mb']:.0f}MB")

# Stream data
for batch in storage.read_auto_format("lineitem.tbl"):
    filtered = batch[batch.iloc[:, 4] > 30]
    print(f"Matched: {len(filtered)} rows")
```



---

## Performance Comparison

### Speed (100MB dataset)
```
Operation              Pandas    DuckDB    Speedup
Statistics (1 col)     0.42s     0.004s    100x
Column selection       0.25s     0.025s    10x
Row filtering          0.19s     0.032s    6x
GROUP BY               0.31s     0.031s    10x
```

### Memory (by dataset size)
```
Dataset    Pandas       DuckDB       Chunked
100MB      300MB        100MB        100MB
1GB        3GB          1GB          100MB
10GB       30GB ‚ùå      3GB ‚úÖ       100MB ‚úÖ
100GB      300GB ‚ùå     30GB ‚ùå      100MB ‚úÖ
```

### File Sizes (10GB data)
```
Format      Size        Compression
Parquet     2-3GB       70% reduction
TBL         10GB        No compression
DAT         10GB        No compression
CSV         12GB        No compression
```

---

## File Structure

```
src/
‚îú‚îÄ‚îÄ processing/
‚îÇ   ‚îú‚îÄ‚îÄ duckdb_processor.py       (440 lines) - SQL engine
‚îÇ   ‚îú‚îÄ‚îÄ chunked_processor.py      (450 lines) - Streaming
‚îÇ   ‚îî‚îÄ‚îÄ row_processor.py          (existing)
‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îú‚îÄ‚îÄ remote_storage.py         (420 lines) - Hetzner API
‚îÇ   ‚îî‚îÄ‚îÄ duckdb_generator.py       (existing)
‚îî‚îÄ‚îÄ config.py                     (360 lines) - Configuration

config/
‚îî‚îÄ‚îÄ processing.yaml               (180 lines) - Settings

run_full_experiment.py            (500 lines) - Orchestration
test_scaling.py                   (480 lines) - 23 tests
```

---

## Troubleshooting

### "uv: command not found"
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### "Module not found"
```bash
uv sync
uv run python script.py
```

### "Out of memory"
```bash
# Use chunked processor
uv run python run_full_experiment.py --processor chunked
```

### "Hetzner connection failed"
```bash
# Check credentials in .env
cat .env

# Test connection
uv run python -c "
from src.config import get_config, HetznerConfig

config = get_config()
hetzner = HetznerConfig(config)
if hetzner.validate():
    print('‚úì Connected')
else:
    print('‚úó Failed')
"
```

---

## Workflow Examples



### Benchmark 100GB from Hetzner
```bash
# Add credentials to .env first
# Enable in config/processing.yaml

uv sync
uv run python run_full_experiment.py \
    --scale-factor 10 \
    --data-source hetzner \
    --processor chunked
```

### Compare Processors
```bash
echo "DuckDB:"
uv run python run_full_experiment.py --processor duckdb

echo "Chunked:"
uv run python run_full_experiment.py --processor chunked

echo "Pandas:"
uv run python run_full_experiment.py --processor pandas
```

### Run Tests
```bash
uv pip install pytest pytest-benchmark
uv run pytest test_scaling.py -v
```

---

## API Reference

### DuckDBProcessor
```python
proc = DuckDBProcessor(data_dir)
proc.register_parquet_table(table_name)
proc.register_all_tables()
proc.select_columns(table, columns)
proc.filter_rows(table, where_clause)
proc.aggregate_rows(table, group_by, agg_spec)
proc.compute_statistics(table, column)
proc.join_tables(left, right, on)
proc.to_pandas(result)
```

### LocalChunkedProcessor
```python
proc = LocalChunkedProcessor(data_dir, chunk_size=100_000)
for batch in proc.read_batches(table):
    pass
result = proc.filter_rows_chunked(table, condition)
stats = proc.compute_statistics_chunked(table, column)
result = proc.aggregate_rows_chunked(table, group_by, agg_spec)
```

### RemoteChunkedProcessor
```python
proc = RemoteChunkedProcessor(storage, chunk_size=100_000)
for batch in proc.read_batches(path):
    pass
```

### HetznerStorage
```python
storage = HetznerStorage(endpoint, access_key, secret_key, bucket)
for batch in storage.read_parquet_batch(path):
    pass
for batch in storage.read_tbl_batch(path):
    pass
for batch in storage.read_dat_batch(path, delimiter='|'):
    pass
for batch in storage.read_auto_format(path):
    pass
storage.upload_file(local_path, remote_path)
storage.download_file(remote_path, local_path)
storage.list_files()
storage.list_datasets()
```

### Config
```python
from src.config import get_config, HetznerConfig

config = get_config()
processor = config.get('processing.processor')
chunk_size = config.get('processing.chunk_size')
config.set('processing.processor', 'chunked')

hetzner = HetznerConfig(config)
if hetzner.is_enabled():
    creds = hetzner.get_credentials()
```

---

## Summary

‚úÖ **Setup:** `uv sync` (2 minutes)
‚úÖ **Run:** `uv run python run_full_experiment.py --processor duckdb`
‚úÖ **Test:** `uv run pytest test_scaling.py -v`
‚úÖ **Scale:** `--scale-factor 1` for 10GB, `--scale-factor 10` for 100GB
‚úÖ **Hetzner:** Add credentials to `.env` and enable in config

Everything else is automatic - formats auto-detect, memory is managed, streaming is built-in.

**That's it!** üöÄ
