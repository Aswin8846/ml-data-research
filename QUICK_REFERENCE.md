# Quick Reference: Project Structure & Key Concepts

## ğŸ“‹ File Organization

```
ml-data-research/
â”‚
â”œâ”€â”€ ğŸ“„ STUDENT_GUIDE.md              â† START HERE! Student-friendly explanation
â”œâ”€â”€ ğŸ“„ PROJECT_OVERVIEW.md           â† Detailed S.T.A.R. framework
â”œâ”€â”€ ğŸ“„ QUICK_REFERENCE.md            â† This file
â”œâ”€â”€ ğŸ“„ README.md                     â† Original project README
â”‚
â”œâ”€â”€ ğŸ”§ run_full_experiment.py        â† Main orchestrator (run this!)
â”œâ”€â”€ ğŸ§ª test_ingestion.py             â† Test data generation
â”œâ”€â”€ ğŸ§ª test_processors.py            â† Test processing operations
â”‚
â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ ingestion/                â† DATA GENERATION
â”‚   â”‚   â”œâ”€â”€ duckdb_generator.py      â”‚ TPCGenerator: generate_tpc_h()
â”‚   â”‚   â”œâ”€â”€ format_converter.py      â”‚ CSV â†” Parquet conversion
â”‚   â”‚   â”œâ”€â”€ kaggle_loader.py         â”‚ Load Kaggle datasets (future)
â”‚   â”‚   â””â”€â”€ cli.py                   â”‚ Command-line interface (future)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ processing/               â† DUAL IMPLEMENTATIONS
â”‚   â”‚   â”œâ”€â”€ row_processor.py         â”‚ RowProcessor: CSV-based operations
â”‚   â”‚   â”œâ”€â”€ column_processor.py      â”‚ ColumnProcessor: Parquet-based operations
â”‚   â”‚   â””â”€â”€ metrics_collector.py     â”‚ MetricsCollector: Performance monitoring
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ analysis/                 â† VISUALIZATION & REPORTING
â”‚       â”œâ”€â”€ visualizer.py            â”‚ PerformanceVisualizer: Create charts
â”‚       â””â”€â”€ report_generator.py      â”‚ ReportGenerator: HTML/Markdown reports
â”‚
â”œâ”€â”€ âš™ï¸ config/
â”‚   â”œâ”€â”€ experiments.yaml             â† Experiment configurations
â”‚   â””â”€â”€ datasets.yaml                â† Dataset definitions
â”‚
â”œâ”€â”€ ğŸ’¾ data/
â”‚   â”œâ”€â”€ raw/                         â† Generated datasets (gitignored)
â”‚   â”‚   â””â”€â”€ tpc_h_sf0.1/
â”‚   â”‚       â”œâ”€â”€ parquet/             â† *.parquet files (~14 MB)
â”‚   â”‚       â””â”€â”€ csv/                 â† *.csv files (~100 MB)
â”‚   â””â”€â”€ processed/                   â† For future processed datasets
â”‚
â””â”€â”€ ğŸ“Š outputs/
    â”œâ”€â”€ metrics/                     â† experiment_results.json
    â”œâ”€â”€ charts/                      â† PNG charts + dashboard.html
    â””â”€â”€ reports/                     â† performance_report.html + SUMMARY.md
```

---

## ğŸ¯ Core Components (Cheat Sheet)

### 1. Data Generation (`src/ingestion/duckdb_generator.py`)
```python
from src.ingestion.duckdb_generator import TPCGenerator

# Create generator
gen = TPCGenerator(output_dir="./data/raw")

# Generate TPC-H benchmark data
paths = gen.generate_tpc_h(scale_factor=0.1, format="parquet")

# Clean up
gen.close()
```

**What it does:** Creates industry-standard benchmark dataset
**Generates:** 8 tables (customer, orders, lineitem, supplier, etc.)
**Size:** ~100 MB at scale factor 0.1

---

### 2. Row-Based Processing (`src/processing/row_processor.py`)
```python
from src.processing.row_processor import RowProcessor

processor = RowProcessor(data_dir="./data/raw/tpc_h_sf0.1/csv")

# Load entire CSV file
df = processor.load_table("lineitem")  # All rows + all columns

# Select columns (must read all, keep some)
result = processor.select_columns("lineitem", 
    columns=['l_quantity', 'l_extendedprice', 'l_discount'])

# Filter rows
result = processor.filter_rows("lineitem", 
    lambda row: row['l_quantity'] > 30)

# Aggregate
result = processor.aggregate_rows("lineitem",
    group_by=['l_returnflag'],
    agg_func={'l_quantity': 'sum', 'l_extendedprice': 'mean'})

# Statistics (reads ALL columns)
stats = processor.compute_statistics("lineitem", "l_extendedprice")
```

**Format:** CSV files (row-oriented)
**Base:** Pandas DataFrames
**Characteristic:** Always reads entire rows, even for partial column access

---

### 3. Column-Based Processing (`src/processing/column_processor.py`)
```python
from src.processing.column_processor import ColumnProcessor

processor = ColumnProcessor(data_dir="./data/raw/tpc_h_sf0.1/parquet")

# Load with SELECTIVE columns
df = processor.load_table("lineitem", 
    columns=['l_quantity', 'l_extendedprice'])  # Read only these!

# Select columns (only loads requested columns)
result = processor.select_columns("lineitem",
    columns=['l_quantity', 'l_extendedprice', 'l_discount'])

# Filter with required_columns hint
result = processor.filter_rows("lineitem",
    condition=lambda row: row['l_quantity'] > 30,
    required_columns=['l_quantity'])  # Load only what's needed

# Aggregate with selective loading
result = processor.aggregate_rows("lineitem",
    group_by=['l_returnflag'],
    agg_func={'l_quantity': 'sum'})  # Loads only: l_returnflag, l_quantity

# Statistics (reads ONLY ONE column!)
stats = processor.compute_statistics("lineitem", "l_extendedprice")  # 1 column
```

**Format:** Parquet files (columnar, compressed)
**Base:** PyArrow Parquet reader
**Characteristic:** Selective column loading â†’ efficiency â­

---

### 4. Metrics Collection (`src/processing/metrics_collector.py`)
```python
from src.processing.metrics_collector import MetricsCollector

collector = MetricsCollector(sample_interval=0.1)  # Sample every 100ms

# Context manager style (recommended)
with collector.measure("operation_name", "dataset", "row") as ctx:
    result = row_processor.select_columns("lineitem", 
        columns=['l_quantity', 'l_extendedprice'])
    ctx.set_rows_processed(len(result))

# Get results
metrics = collector.get_latest_metrics()
print(f"Duration: {metrics.duration_seconds:.3f}s")
print(f"Peak Memory: {metrics.max_memory_mb:.1f} MB")
print(f"Avg CPU: {metrics.avg_cpu:.1f}%")
print(f"Disk Read: {metrics.total_disk_read_mb:.2f} MB")

# Save to JSON
collector.save_metrics(Path("outputs/metrics/results.json"))
```

**Collects:** CPU, Memory, Disk I/O, Timestamp
**Method:** Background thread (non-blocking)
**Frequency:** Configurable (default 100ms)
**Output:** JSON with full metrics + snapshots

---

### 5. Visualization (`src/analysis/visualizer.py`)
```python
from src.analysis.visualizer import PerformanceVisualizer

visualizer = PerformanceVisualizer(
    metrics_file=Path("outputs/metrics/experiment_results.json"),
    output_dir=Path("outputs/charts")
)

# Generate all charts
charts = visualizer.generate_all_visualizations()

# Individual charts available:
visualizer.create_duration_comparison()      # Bar chart: time comparison
visualizer.create_memory_comparison()        # Bar chart: memory usage
visualizer.create_io_comparison()            # Bar chart: disk I/O
visualizer.create_performance_heatmap()      # Heatmap: speedup factors
visualizer.create_time_series_plot()         # 4-panel time series
visualizer.create_interactive_dashboard()    # Interactive Plotly dashboard
```

**Output Formats:** PNG (static), HTML (interactive)
**Charts Generated:** 6 types covering all metrics
**Output Location:** `outputs/charts/`

---

### 6. Reporting (`src/analysis/report_generator.py`)
```python
from src.analysis.report_generator import ReportGenerator

generator = ReportGenerator(
    metrics_file=Path("outputs/metrics/experiment_results.json"),
    charts_dir=Path("outputs/charts"),
    output_dir=Path("outputs/reports")
)

# Generate all reports
reports = generator.generate_all_reports()

# Individual reports:
generator.generate_html_report()           # Professional HTML report
generator.generate_markdown_summary()      # Quick markdown reference

# Calculate statistics
summary = generator.calculate_summary_statistics()
print(f"Average speedup: {summary['avg_speedup']:.2f}x")
print(f"Total operations: {summary['total_operations']}")
```

**HTML Report Features:**
- Executive summary with KPIs
- Detailed metrics table
- Embedded visualizations
- Methodology section
- Conclusions & recommendations

---

## ğŸ”„ Main Pipeline (`run_full_experiment.py`)

```python
# Step 1: Generate Data (if needed)
generator = TPCGenerator(output_dir=data_dir)
generator.generate_tpc_h(scale_factor=0.1, format="parquet")
generator.generate_tpc_h(scale_factor=0.1, format="csv")
generator.close()

# Step 2: Initialize Processors & Collector
row_processor = RowProcessor(data_dir=csv_dir)
col_processor = ColumnProcessor(data_dir=parquet_dir)
collector = MetricsCollector(sample_interval=0.1)

# Step 3: Run Operations (4 operations Ã— 2 formats = 8 measurements)
operations = [
    {
        'name': 'select_columns',
        'description': 'Select specific columns',
        'row_func': lambda: row_processor.select_columns(...),
        'col_func': lambda: col_processor.select_columns(...)
    },
    # ... more operations
]

for op in operations:
    # Measure row-based
    with collector.measure(op['name'], "lineitem", "row") as ctx:
        result = op['row_func']()
        ctx.set_rows_processed(len(result))
    
    # Measure column-based
    with collector.measure(op['name'], "lineitem", "column") as ctx:
        result = op['col_func']()
        ctx.set_rows_processed(len(result))

# Step 4: Generate Visualizations
visualizer = PerformanceVisualizer(metrics_file, output_dir)
visualizer.generate_all_visualizations()

# Step 5: Generate Reports
report_gen = ReportGenerator(metrics_file, charts_dir, output_dir)
report_gen.generate_all_reports()
```

---

## ğŸ“Š Data Flow Diagram

```
Input: TPC-H Benchmark (100MB)
         â”‚
         â”œâ”€ CSV Version (100 MB, row-oriented)
         â”‚   â””â”€ RowProcessor
         â”‚       â”œâ”€ load_table() â†’ Read all columns
         â”‚       â”œâ”€ select_columns() â†’ 0.245s âŒ
         â”‚       â”œâ”€ filter_rows() â†’ 0.189s
         â”‚       â”œâ”€ aggregate_rows() â†’ 0.312s
         â”‚       â””â”€ compute_statistics() â†’ 0.423s âŒ
         â”‚
         â””â”€ Parquet Version (14 MB, column-based)
             â””â”€ ColumnProcessor
                 â”œâ”€ load_table(columns=[...]) â†’ Read selected columns only
                 â”œâ”€ select_columns() â†’ 0.082s âœ… (3.0x faster!)
                 â”œâ”€ filter_rows() â†’ 0.102s
                 â”œâ”€ aggregate_rows() â†’ 0.100s
                 â””â”€ compute_statistics() â†’ 0.100s âœ… (4.2x faster!)

         â†“ (Both paths)
         
    MetricsCollector
         â”‚
         â”œâ”€ CPU usage (%)
         â”œâ”€ Memory (MB)
         â”œâ”€ Disk I/O (MB)
         â””â”€ Execution time (s)
         
         â†“
         
    PerformanceVisualizer
         â”‚
         â”œâ”€ Duration comparison chart
         â”œâ”€ Memory comparison chart
         â”œâ”€ I/O comparison chart
         â”œâ”€ Performance heatmap
         â”œâ”€ Time series plots
         â””â”€ Interactive dashboard
         
         â†“
         
    ReportGenerator
         â”‚
         â”œâ”€ HTML Report (embedded charts + styling)
         â”œâ”€ Markdown Summary
         â””â”€ JSON Metrics (raw data)
```

---

## ğŸ§ª Testing Commands

```bash
# Test data generation
python test_ingestion.py

# Test processing operations
python test_processors.py

# Run full experiment
python run_full_experiment.py
```

---

## ğŸ“Š Expected Results Summary

| Metric | Row-Based | Column-Based | Improvement |
|--------|-----------|--------------|-------------|
| **Execution Time** | 1.169s | 0.384s | **3.04x faster** |
| **Peak Memory** | 176.2 MB | 61.3 MB | **65% saved** |
| **Disk I/O** | 47.3 MB | 10.6 MB | **78% reduction** |
| **Storage** | 100 MB | 14 MB | **86% compression** |

---

## ğŸ¯ Key Concepts at a Glance

| Concept | Row-Based (CSV) | Column-Based (Parquet) |
|---------|-----------------|----------------------|
| **Storage** | ~100 MB | ~14 MB (86% smaller!) |
| **Reading model** | Entire rows | Specific columns |
| **Column selection** | âŒ Slow (reads all) | âœ… Fast (reads requested only) |
| **Statistics** | âŒ Slow (all columns) | âœ… Fast (one column) |
| **Compression** | Basic | Dictionary + RLE + Snappy |
| **Best for** | Transactional (OLTP) | Analytical (OLAP) |
| **Cloud storage** | âŒ Inefficient | âœ… Efficient streaming |

---

## ğŸ’¾ Dependency Overview

```
Core Data Processing:
â”œâ”€ duckdb (benchmark generation)
â”œâ”€ pandas (data manipulation)
â”œâ”€ pyarrow (columnar format)
â””â”€ polars (fast processing, extensible)

Performance Monitoring:
â”œâ”€ psutil (CPU/memory)
â”œâ”€ py-cpuinfo (CPU details)
â””â”€ threading (background monitoring)

Visualization:
â”œâ”€ matplotlib (static charts)
â”œâ”€ seaborn (statistical graphics)
â””â”€ plotly (interactive dashboards)

Reporting:
â”œâ”€ jinja2 (HTML templating)
â”œâ”€ markdown (format support)
â””â”€ weasyprint (PDF, future)

Utilities:
â”œâ”€ click (CLI framework)
â”œâ”€ rich (terminal output)
â”œâ”€ yaml (configuration)
â””â”€ python-dotenv (.env files)
```

---

## ğŸš€ Running in 30 Seconds

```bash
# 1. Install (one-time)
pip install -r requirements.txt

# 2. Run (2-3 minutes)
python run_full_experiment.py

# 3. View (instantly)
open outputs/reports/performance_report.html
```

Done! You now have a complete benchmarking study with professional reports.

---

## â“ Troubleshooting

| Issue | Solution |
|-------|----------|
| DuckDB extension not found | Run once to cache the TPC-H data |
| Out of memory | Reduce scale_factor (0.05 instead of 0.1) |
| Charts not generating | Ensure matplotlib backend is set |
| Permission denied | Use `python -m` or check file permissions |
| Slow performance | First run generates data (2-3 min), subsequent runs are instant |

---

## ğŸ“š File Types in Outputs

```
outputs/
â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ experiment_results.json          [Raw metrics, JSON format]
â”‚
â”œâ”€â”€ charts/
â”‚   â”œâ”€â”€ duration_comparison.png          [Bar chart, PNG]
â”‚   â”œâ”€â”€ memory_comparison.png            [Bar chart, PNG]
â”‚   â”œâ”€â”€ io_comparison.png                [Bar chart, PNG]
â”‚   â”œâ”€â”€ performance_heatmap.png          [Heatmap, PNG]
â”‚   â”œâ”€â”€ time_series.png                  [4-panel plot, PNG]
â”‚   â””â”€â”€ interactive_dashboard.html       [Interactive, Plotly]
â”‚
â””â”€â”€ reports/
    â”œâ”€â”€ performance_report.html          [Main report, styled HTML]
    â””â”€â”€ SUMMARY.md                       [Quick reference, Markdown]
```

---

**Next Steps:** Read STUDENT_GUIDE.md for the complete explanation!
